from data2 import  ff_mask ,ff_data
from Xcept2 import Xception
from ca import CA_Block

import time
import torch
from torchvision import datasets, models, transforms
import os
import torch.nn as nn

import torch.optim as optim
import torchvision
from torch.optim import lr_scheduler
from torch.utils.data import DataLoader ,Dataset , random_split
import random
import numpy as np
import joblib
from efficientnet_pytorch import EfficientNet
from PIL import Image
from ipywidgets import IntProgress
from pytorchtools2 import EarlyStopping
from torchsummary import summary
from torch.utils.data import  BatchSampler , SequentialSampler , RandomSampler


augmentation = torchvision.transforms.Compose([
                                                    #torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),
                                                    torchvision.transforms.RandomRotation((90), expand=True),
                                                    torchvision.transforms.Resize(160),
                                                    torchvision.transforms.RandomCrop(160),                                                                            
                                                    torchvision.transforms.RandomHorizontalFlip(),
                                                    torchvision.transforms.ToTensor(),
                                                    torchvision.transforms.Normalize([0.5, 0.5, 0.5],[0.5, 0.5, 0.5])
                                                    ])

augmentation2 = torchvision.transforms.Compose([
                                                    torchvision.transforms.Resize(160),
                                                    torchvision.transforms.ToTensor(),
                                                    torchvision.transforms.Normalize([0.5, 0.5, 0.5],[0.5, 0.5, 0.5])
                                                    ])

def adjust_learning_rate(optimizer, lr):
    """Sets the learning rate to the initial LR decayed by 10 every 30 epochs"""
    
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr


def train_model( train_loader, val_loader , model , criterion , optimizer  , num_epochs , patience , learning_rate=0.001 ):
    
    early_stopping = EarlyStopping(patience=patience, verbose=True)
    
    
    
    for epoch in range(num_epochs):
        
        print('Epoch : {}/{}'.format(epoch, num_epochs - 1))
        print('-' * 10)
        train_accuracy = 0.0
        val_accuracy = 0.0
        train_loss = 0.0
        val_loss = 0.0
        
        model.train()
        
        for inputs, labels in train_loader  :
            inputs, labels = inputs.to(device , dtype=torch.float32), labels.to(device)
            
            optimizer.zero_grad()          
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            loss.backward()
            optimizer.step()
            
            _, pred = torch.max(torch.sigmoid(outputs), 1)
            
            acc = torch.sum(pred==labels).item()
            train_accuracy += acc
            
            train_loss += loss.item()
        
        print("Train Loss: {}".format(train_loss / len(train_loader) ) )
        print("Train Accuracy: {}".format(train_accuracy / len(c)) )
        print()
        print(learning_rate)
   
        if epoch %10 == 9 :
            learning_rate = learning_rate*0.8
            adjust_learning_rate(optimizer , learning_rate)
            
            
        model.eval()
        with torch.no_grad():
            for inputs, labels in val_loader :
                inputs, labels = inputs.to(device , dtype=torch.float32), labels.to(device)
                outputs = model(inputs)
                
                _, pred = torch.max(torch.sigmoid(outputs), 1)
                
                
                val_accuracy += torch.sum(pred==labels).item()
                loss = criterion(outputs, labels)
                val_loss += loss.item()
        
            print("val Loss: {}".format(val_loss / len(val_loader) ))    
            print("val Accuracy: {}".format(val_accuracy / len(val) ))
            print()
            
        early_stopping( (val_loss / len(val_loader) ), model)
        if early_stopping.early_stop :
            print("Early stopping!!")
            break
         
    return model




class CA(nn.Module):
    def __init__(self, modelA, nb_classes=2):
        super(CA, self).__init__()
        self.modelA = modelA
        
        # Remove last linear layer
        self.modelA._fc = nn.Identity()
        
        self.ca_model = CA_Block(channel=2048, h=5, w=5)
        
        # Create new classifier
        self.classifier = nn.Linear(2048, nb_classes)
        
        
    def forward(self, x ):
        x = self.modelA.extract_features(x) 
        
        y = self.ca_model(x)
        x = torch.mul(y,x)
        
        x = self.modelA._avg_pooling(x)
        x = x.view(x.size(0), -1)
        x = self.modelA._dropout(x)
        
        x = self.classifier(x)
        x = self.modelA._swish(x)
        return x




if __name__ =="__main__":

    model = EfficientNet.from_pretrained('efficientnet-b5' , num_classes=2)
    model = CA( model )
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    criterion = nn.CrossEntropyLoss()
    learning_rate = 0.0002
    optimizer_ft = optim.Adam(model.parameters(), lr= learning_rate )
    
# load dataset =========================================================================
    Batch_size = 8

    m = ff_mask(transforms = augmentation2 , batch = 8)
    f = ff_data(transforms = augmentation , mode = 0 , batch = 8) # mode0 : train , mode1 : test , mode2 : val
    c = m+f
    

    train_loader = DataLoader(c ,batch_size =Batch_size , shuffle = True)
    
    val = ff_data(transforms = augmentation2 , mode = 2 , batch = Batch_size )
    
    l2 = list(BatchSampler(SequentialSampler(range(len(val))), batch_size= Batch_size , drop_last=False))
    
    val_loader = DataLoader(val ,batch_sampler = l2)


    
    model = train_model(
                        train_loader, 
                        val_loader , 
                        model, 
                        criterion, 
                        optimizer_ft , 
                        patience = 30, 
                        num_epochs=300 , 
                        learning_rate = learning_rate
                        )    
    
    #summary(model , (3,200,200))
